{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database schema created successfully\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "\n",
    "conn.database = database\n",
    "    \n",
    "with open('schema2.sql', 'r') as schema:\n",
    "    sql_script = schema.read()\n",
    "\n",
    "try:\n",
    "\n",
    "    for statement in sql_script.split(\";\\n\"):\n",
    "        statement = statement.strip()\n",
    "        if statement:\n",
    "            cursor.execute(statement)\n",
    "except mysql.Error as err:\n",
    "    print(f\"Error executing statement: {statement}\")\n",
    "    print(f\"MySQL Error: {err}\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print('Database schema created successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Wildfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\pandas_datasets.py:91: DtypeWarning: Columns (14,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  result = read_function(\n",
      "100%|██████████| 386910/386910 [00:14<00:00, 27072.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "use_cols = ['OBJECTID', 'FIRE_NAME', 'DISCOVERY_DATE', 'NWCG_GENERAL_CAUSE', 'FIRE_SIZE', 'STATE', 'FIPS_NAME']\n",
    "\n",
    "# Download latest version\n",
    "wildfire_df = kagglehub.load_dataset(handle = \"behroozsohrabi/us-wildfire-records-6th-edition\", path = \"data.csv\", \n",
    "                                   adapter = KaggleDatasetAdapter.PANDAS, \n",
    "                                   pandas_kwargs={\"usecols\": use_cols, \"compression\": \"zip\"})\n",
    "\n",
    "\n",
    "# Filter states that we're keeping\n",
    "\n",
    "states_to_keep = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "wildfire_df.loc[~wildfire_df['STATE'].isin(states_to_keep), :] = None\n",
    "\n",
    "wildfire_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Convert DISCOVERY_DATE to datetime object to remove records prior to year 2000\n",
    "\n",
    "wildfire_df['DISCOVERY_DATE'] = pd.to_datetime(wildfire_df['DISCOVERY_DATE'], format = ('%m/%d/%Y'))\n",
    "\n",
    "wildfire_df = wildfire_df[wildfire_df['DISCOVERY_DATE'].dt.year > 2000]\n",
    "\n",
    "\n",
    "# Change to int for compatiabilty with Primary Key\n",
    "\n",
    "wildfire_df['OBJECTID'] = wildfire_df['OBJECTID'].astype(int)\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "\n",
    "wildfire_df = wildfire_df.rename(columns = {\"NWCG_GENERAL_CAUSE\": \"SPECIFIC_CAUSE\", \"FIPS_NAME\": \"COUNTY\"})\n",
    "\n",
    "\n",
    "# Call script to map state FIPS codes to new columns\n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "wildfire_df['state_id'] = wildfire_df['STATE'].map(state_fips_mapping)\n",
    "\n",
    "\n",
    "# Realign columns and load location data to prep for county_id mapping\n",
    "\n",
    "wildfire_df.rename(columns = {'COUNTY': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations2.csv\")\n",
    "\n",
    "\n",
    "# Map county codes to wildfire data\n",
    "\n",
    "wildfire_df = match_county_id(wildfire_df, locations_df)\n",
    "\n",
    "\n",
    "# Use missing values scripts to fill anything missing\n",
    "\n",
    "%run Scripts/missing_values.py\n",
    "\n",
    "missing_dict = {\n",
    "    \"FL\": 86,\n",
    "    \"GA\": 29\n",
    "}\n",
    "\n",
    "wildfire_df = fill_missing_values(wildfire_df, missing_dict, \"STATE\", \"county_id\")\n",
    "\n",
    "\n",
    "# Connect to database\n",
    "\n",
    "%run Scripts/connect2.py\n",
    "\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "query = \"SELECT location_id, state_id, county_id FROM locations\"\n",
    "cursor.execute(query)\n",
    "location_map = pd.DataFrame(cursor.fetchall(), columns=['location_id', 'state_id', 'county_id'])\n",
    "\n",
    "wildfire_df = wildfire_df.merge(location_map[['location_id', 'state_id', 'county_id']],\n",
    "                                    on=['state_id', 'county_id'], how='left')\n",
    "\n",
    "columns = ['OBJECTID', 'location_id', 'FIRE_NAME', 'DISCOVERY_DATE', 'SPECIFIC_CAUSE', 'FIRE_SIZE']\n",
    "\n",
    "data_to_insert = list(wildfire_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO wildfire (fire_id, location_id, fire_name, discovery_date, cause, fire_size)\n",
    "VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10000\n",
    "start_index = 0\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        cursor.executemany(clause, batch)\n",
    "        conn.commit()\n",
    "        print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully\n",
      "Data unable to be loaded: Failed executing the operation; Not all parameters were used in the SQL statement\n"
     ]
    }
   ],
   "source": [
    "# Load location data from census website\n",
    "\n",
    "ca_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st06_ca_place_by_county2020.txt', delimiter = '|')\n",
    "tx_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st48_tx_place_by_county2020.txt', delimiter = '|')\n",
    "ga_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st13_ga_place_by_county2020.txt', delimiter = '|')\n",
    "fl_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st12_fl_place_by_county2020.txt', delimiter = '|')\n",
    "az_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st04_az_place_by_county2020.txt', delimiter = '|')\n",
    "\n",
    "\n",
    "# Combine data\n",
    "\n",
    "combined_df = pd.concat([ca_codes, tx_codes, ga_codes, fl_codes, az_codes], axis = 0, ignore_index=True)\n",
    "\n",
    "\n",
    "#Drop unused columns\n",
    "\n",
    "columns_to_drop = ['PLACENS', 'TYPE', 'CLASSFP', 'FUNCSTAT']\n",
    "\n",
    "combined_df.drop(columns = columns_to_drop, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns for mapping, and load updated data to folder for reference for other tables\n",
    "\n",
    "combined_df.rename(columns = {'COUNTYNAME': 'county_name', 'COUNTYFP': 'county_id', 'STATEFP': 'state_id'}, inplace = True)\n",
    "\n",
    "combined_df.to_csv(\"data/locations2.csv\")\n",
    "\n",
    "\n",
    "# Add separate location data into dict for syncing\n",
    "\n",
    "location_dict = {\n",
    "    'CA': ca_codes,\n",
    "    'TX': tx_codes,\n",
    "    'GA': ga_codes,\n",
    "    'FL': fl_codes,\n",
    "    'AZ': az_codes\n",
    "}\n",
    "\n",
    "# Load population data from census website\n",
    "\n",
    "\"\"\"state_df = pd.read_csv(\"/your path.csv\")\"\"\"\n",
    "\n",
    "\n",
    "# Add states to dictionary to make transforming easier\n",
    "\n",
    "state_population_dict = {}\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "for state in states:\n",
    "    state_population_dict[state] = pd.read_csv(f\"data/{state} City population estimates.csv\")\n",
    "\n",
    "\n",
    "# Sync location and population data to standardize state, county and place codes\n",
    "\n",
    "%run Scripts/sync.py\n",
    "\n",
    "sync(states, state_population_dict, location_dict)\n",
    "\n",
    "\n",
    "# Combine state_population_dict for loading\n",
    "\n",
    "population_df = pd.concat(state_population_dict.values(), ignore_index = True)\n",
    "\n",
    "\n",
    "# Replace any A values in official Census feature\n",
    "\n",
    "population_df['CENSUS2010POP'] = population_df['CENSUS2010POP'].apply(lambda x: 0 if x == 'A' else x)\n",
    "\n",
    "population_df.rename(columns = {\"STATE\": \"state_id\", \"COUNTYFP\": \"county_id\"}, inplace = True)\n",
    "combined_df.rename(columns = {\"STATE\": \"state_name\"}, inplace = True)\n",
    "\n",
    "\n",
    "# Connect and load location and population data\n",
    "\n",
    "%run Scripts/connect2.py\n",
    "\n",
    "\n",
    "# Location loading script\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = ['state_name', 'state_id', 'county_id', 'county_name']\n",
    "\n",
    "data_to_insert = list(combined_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "insert_clause = \"\"\"\n",
    "INSERT IGNORE INTO locations (state_name, state_id, county_id, county_name)\n",
    "Values (%s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(insert_clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "\n",
    "# Grab location_id\n",
    "\n",
    "query = \"SELECT location_id, state_id, county_id FROM locations\"\n",
    "cursor.execute(query)\n",
    "location_map = pd.DataFrame(cursor.fetchall(), columns=['location_id', 'state_id', 'county_id'])\n",
    "\n",
    "population_df = population_df.merge(location_map[['location_id', 'state_id', 'county_id']],\n",
    "                                    on=['state_id', 'county_id'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Population loading script\n",
    "\n",
    "data_to_insert = list(population_df[['location_id', 'CENSUS2010POP', 'POPESTIMATE2011', 'POPESTIMATE2012', 'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015',\n",
    "                                     'POPESTIMATE2016', 'POPESTIMATE2017', 'POPESTIMATE2018','POPESTIMATE2019']].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO census (location_id, true_pop_2010, pop_estimate_2011, pop_estimate_2012, pop_estimate_2013, pop_estimate_2014, pop_estimate_2015, \n",
    "pop_estimate_2016, pop_estimate_2017, pop_estimate_2018, pop_estimate_2019)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Zillow data\n",
    "\n",
    "\"\"\"df = pd.read_csv(\"/your path.csv\")\"\"\"\n",
    "\n",
    "\n",
    "# To parquet if needed for on-site storage or to load to github\n",
    "\n",
    "\"\"\"df.to_parquet(\"/your path.parquet)\"\"\"\n",
    "\n",
    "\n",
    "# Load and remove columns that aren't used\n",
    "\n",
    "bottom_tier_housing = pd.read_parquet('data/bottom_housing.parquet')\n",
    "top_tier_housing = pd.read_parquet('data/top_housing.parquet')\n",
    "\n",
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "\n",
    "bottom_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "top_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "\n",
    "\n",
    "# Filter out states not using and remove na values\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "bottom_tier_housing.loc[~bottom_tier_housing['State'].isin(states), :] = None\n",
    "top_tier_housing.loc[~top_tier_housing['State'].isin(states), :] = None\n",
    "\n",
    "bottom_tier_housing.dropna(inplace = True)\n",
    "top_tier_housing.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Combine the data frames and pivot long the date features and price values and change date column to date dtype\n",
    "\n",
    "housing_df = pd.concat([bottom_tier_housing, top_tier_housing], axis = 0, ignore_index = True)\n",
    "\n",
    "housing_df = housing_df.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "housing_df['Date'] = pd.to_datetime(housing_df['Date']).dt.date\n",
    "\n",
    "\n",
    "# Map state and county id codes \n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "housing_df['state_id'] = housing_df['State'].map(state_fips_mapping)\n",
    "\n",
    "housing_df.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations2.csv\")\n",
    "\n",
    "housing_df = match_county_id(housing_df, locations_df)\n",
    "\n",
    "\n",
    "# Map location_id\n",
    "\n",
    "location_map = pd.read_csv('data/location_map.csv')\n",
    "\n",
    "housing_df = housing_df.merge(location_map[['location_id', 'state_id', 'county_id']],\n",
    "                                    on=['state_id', 'county_id'], how='left')\n",
    "\n",
    "\n",
    "# Connect and load data\n",
    "\n",
    "%run Scripts/connect2.py\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "columns = ['location_id', 'RegionName', 'Date', 'Price']\n",
    "\n",
    "data_to_insert = list(housing_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO housing (location_id, region_name, assessment_date, price)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "start_index = 0\n",
    "\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == '2013 (HY000): Lost connection to MySQL server during query':\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                #reconnect()\n",
    "                cursor.executemany(clause, batch)  # Retry query\n",
    "                conn.commit()\n",
    "\n",
    "except Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Rental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully for batch 1\n",
      "Data loaded successfully for batch 2\n",
      "Data loaded successfully for batch 3\n"
     ]
    }
   ],
   "source": [
    "# Read in rental data\n",
    "\n",
    "rent_index = pd.read_csv(\"data/Observed Rent Index by City.csv\")\n",
    "\n",
    "\n",
    "# Remove unused columns and filter out states\n",
    "\n",
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "rent_index.drop(columns = remove_cols, inplace = True)\n",
    "rent_index.loc[~rent_index['State'].isin(states), :] = None\n",
    "\n",
    "rent_index.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Pivot long date features and price values and change date column to date dtype\n",
    "\n",
    "rent_index = rent_index.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "rent_index['Date'] = pd.to_datetime(rent_index['Date']).dt.date\n",
    "\n",
    "\n",
    "# Map state and county id codes \n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "rent_index['state_id'] = rent_index['State'].map(state_fips_mapping)\n",
    "\n",
    "rent_index.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "rent_index = match_county_id(rent_index, locations_df)\n",
    "\n",
    "\n",
    "# Map location_id's\n",
    "\n",
    "location_map = pd.read_csv('data/location_map.csv')\n",
    "\n",
    "rent_index = rent_index.merge(location_map[['location_id', 'state_id', 'county_id']],\n",
    "                                    on=['state_id', 'county_id'], how='left')\n",
    "\n",
    "\n",
    "# Connect and load rental data into database\n",
    "\n",
    "%run Scripts/connect2.py\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = ['location_id', 'RegionName', 'Date', 'Price']\n",
    "\n",
    "data_to_insert = list(rent_index[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO rentals (location_id, region_name, assessment_date, price)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == \"2013 (HY000): Lost connection to MySQL server during query\":\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)\n",
    "                conn.commmit()\n",
    "except Error as err:\n",
    "    print(err)\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
