{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmysql\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleDatasetAdapter\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.3.7\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m  \u001b[38;5;66;03m# configures the library logger.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colab_cache_resolver, http_resolver, kaggle_cache_resolver, registry\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login, whoami\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompetition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m competition_download\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\colab_cache_resolver.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclients\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColabClient\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_colab_cache_disabled\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendError, NotFoundError\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\kagglehub\\clients.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m urljoin, urlparse\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\__init__.py:164\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packages, utils\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__version__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    153\u001b[0m     __author__,\n\u001b[0;32m    154\u001b[0m     __author_email__,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     __version__,\n\u001b[0;32m    163\u001b[0m )\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m delete, get, head, options, patch, post, put, request\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;167;01mConnectionError\u001b[39;00m,\n\u001b[0;32m    167\u001b[0m     ConnectTimeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     URLRequired,\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreparedRequest, Request, Response\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mrequests.api\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:license: Apache2, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sessions\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs and sends a :class:`Request <Request>`.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m      <Response [200]>\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_native_string\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madapters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPAdapter\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _basic_auth_str\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mapping, cookielib, urljoin, urlparse\n",
      "File \u001b[1;32mc:\\Users\\Taylor\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:81\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mssl\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     _preloaded_ssl_context \u001b[38;5;241m=\u001b[39m create_urllib3_context()\n\u001b[1;32m---> 81\u001b[0m     \u001b[43m_preloaded_ssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract_zipped_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEFAULT_CA_BUNDLE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Bypass default SSLContext creation when Python\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# interpreter isn't built with the ssl module.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     _preloaded_ssl_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Database create or already exists\n",
      "Database schema created successfully\n"
     ]
    }
   ],
   "source": [
    "# %run Scripts/schema_creation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['OBJECTID', 'FIRE_NAME', 'DISCOVERY_DATE', 'NWCG_GENERAL_CAUSE', 'FIRE_SIZE', 'STATE', 'FIPS_NAME']\n",
    "\n",
    "# Download latest version\n",
    "wildfire_df = kagglehub.load_dataset(handle = \"behroozsohrabi/us-wildfire-records-6th-edition\", path = \"data.csv\", \n",
    "                                   adapter = KaggleDatasetAdapter.PANDAS, \n",
    "                                   pandas_kwargs={\"usecols\": use_cols, \"compression\": \"zip\"})\n",
    "\n",
    "\n",
    "# Filter states that we're keeping\n",
    "\n",
    "states_to_keep = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "wildfire_df.loc[~wildfire_df['STATE'].isin(states_to_keep), :] = None\n",
    "\n",
    "wildfire_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Convert DISCOVERY_DATE to datetime object to remove records prior to year 2000\n",
    "\n",
    "wildfire_df['DISCOVERY_DATE'] = pd.to_datetime(wildfire_df['DISCOVERY_DATE'], format = ('%m/%d/%Y'))\n",
    "\n",
    "wildfire_df = wildfire_df[wildfire_df['DISCOVERY_DATE'].dt.year > 2000]\n",
    "\n",
    "\n",
    "# Change to int for compatiabilty with Primary Key\n",
    "\n",
    "wildfire_df['OBJECTID'] = wildfire_df['OBJECTID'].astype(int)\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "\n",
    "wildfire_df = wildfire_df.rename(columns = {\"NWCG_GENERAL_CAUSE\": \"SPECIFIC_CAUSE\", \"FIPS_NAME\": \"COUNTY\"})\n",
    "\n",
    "\n",
    "# Call script to map state FIPS codes to new columns\n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "wildfire_df['state_id'] = wildfire_df['STATE'].map(state_fips_mapping)\n",
    "\n",
    "\n",
    "# Realign columns and load location data to prep for county_id mapping\n",
    "\n",
    "wildfire_df.rename(columns = {'COUNTY': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "\n",
    "# Map county codes to wildfire data\n",
    "\n",
    "wildfire_df = match_county_id(wildfire_df, locations_df)\n",
    "\n",
    "\n",
    "# Use missing values scripts to fill anything missing\n",
    "\n",
    "%run Scripts/missing_values.py\n",
    "\n",
    "missing_dict = {\n",
    "    \"FL\": 86,\n",
    "    \"GA\": 29\n",
    "}\n",
    "\n",
    "wildfire_df = fill_missing_values(wildfire_df, missing_dict, \"STATE\", \"county_id\")\n",
    "\n",
    "\n",
    "# Connect to database\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = wildfire_df.columns\n",
    "\n",
    "data_to_insert = list(wildfire_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO wildfire (fire_id, fire_name, discovery_date, cause, fire_size, state_name, county_name, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10000\n",
    "start_index = 0\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        cursor.executemany(clause, batch)\n",
    "        conn.commit()\n",
    "        print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994800/994800 [01:01<00:00, 16259.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully for batch 1\n",
      "Data loaded successfully for batch 2\n",
      "Data loaded successfully for batch 3\n",
      "Data loaded successfully for batch 4\n",
      "Data loaded successfully for batch 5\n",
      "Data loaded successfully for batch 6\n",
      "Data loaded successfully for batch 7\n",
      "Data loaded successfully for batch 8\n",
      "Data loaded successfully for batch 9\n",
      "Data loaded successfully for batch 10\n"
     ]
    }
   ],
   "source": [
    "# Download Zillow data\n",
    "\n",
    "\"\"\"df = pd.read_csv(\"/your path.csv\")\"\"\"\n",
    "\n",
    "\n",
    "# To parquet if needed for on-site storage or to load to github\n",
    "\n",
    "\"\"\"df.to_parquet(\"/your path.parquet)\"\"\"\n",
    "\n",
    "\n",
    "# Load and remove columns that aren't used\n",
    "\n",
    "bottom_tier_housing = pd.read_parquet('data/bottom_housing.parquet')\n",
    "top_tier_housing = pd.read_parquet('data/top_housing.parquet')\n",
    "\n",
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "\n",
    "bottom_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "top_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "\n",
    "\n",
    "# Filter out states not using and remove na values\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "bottom_tier_housing.loc[~bottom_tier_housing['State'].isin(states), :] = None\n",
    "top_tier_housing.loc[~top_tier_housing['State'].isin(states), :] = None\n",
    "\n",
    "bottom_tier_housing.dropna(inplace = True)\n",
    "top_tier_housing.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Combine the data frames and pivot long the date features and price values and change date column to date dtype\n",
    "\n",
    "housing_df = pd.concat([bottom_tier_housing, top_tier_housing], axis = 0, ignore_index = True)\n",
    "\n",
    "housing_df = housing_df.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "housing_df['Date'] = pd.to_datetime(housing_df['Date']).dt.date\n",
    "\n",
    "\n",
    "# Map state and county id codes \n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "housing_df['state_id'] = housing_df['State'].map(state_fips_mapping)\n",
    "\n",
    "housing_df.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "housing_df = match_county_id(housing_df, locations_df)\n",
    "\n",
    "\n",
    "# Connect and load data\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = housing_df.columns\n",
    "\n",
    "data_to_insert = list(housing_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO housing (region_name, state_name, county_name, assessment_date, price, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "start_index = 0\n",
    "\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == '2013 (HY000): Lost connection to MySQL server during query':\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)  # Retry query\n",
    "                conn.commit()\n",
    "\n",
    "except Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in rental data\n",
    "\n",
    "rent_index = pd.read_csv(\"data/Observed Rent Index by City.csv\")\n",
    "\n",
    "\n",
    "# Remove unused columns and filter out states\n",
    "\n",
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "rent_index.drop(columns = remove_cols, inplace = True)\n",
    "rent_index.loc[~rent_index['State'].isin(states), :] = None\n",
    "\n",
    "rent_index.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Pivot long date features and price values and change date column to date dtype\n",
    "\n",
    "rent_index = rent_index.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "rent_index['Date'] = pd.to_datetime(housing_df['Date']).dt.date\n",
    "\n",
    "\n",
    "# Map state and county id codes \n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "rent_index['state_id'] = rent_index['State'].map(state_fips_mapping)\n",
    "\n",
    "rent_index.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "rent_index = match_county_id(rent_index, locations_df)\n",
    "\n",
    "\n",
    "# Connect and load rental data into database\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = rent_index.columns\n",
    "\n",
    "data_to_insert = list(rent_index[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO rentals (region_name, state_name, county_name, assessment_date, price, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == \"2013 (HY000): Lost connection to MySQL server during query\":\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)\n",
    "                conn.commmit()\n",
    "except Error as err:\n",
    "    print(err)\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location and Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load location data from census website\n",
    "\n",
    "ca_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st06_ca_place_by_county2020.txt', delimiter = '|')\n",
    "tx_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st48_tx_place_by_county2020.txt', delimiter = '|')\n",
    "ga_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st13_ga_place_by_county2020.txt', delimiter = '|')\n",
    "fl_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st12_fl_place_by_county2020.txt', delimiter = '|')\n",
    "az_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st04_az_place_by_county2020.txt', delimiter = '|')\n",
    "\n",
    "\n",
    "# Combine data\n",
    "\n",
    "combined_df = pd.concat([ca_codes, tx_codes, ga_codes, fl_codes, az_codes], axis = 0, ignore_index=True)\n",
    "\n",
    "\n",
    "#Drop unused columns\n",
    "\n",
    "columns_to_drop = ['PLACENS', 'TYPE', 'CLASSFP', 'FUNCSTAT']\n",
    "\n",
    "combined_df.drop(columns = columns_to_drop, inplace = True)\n",
    "\n",
    "\n",
    "# Rename columns for mapping, and load updated data to folder for reference for other tables\n",
    "\n",
    "combined_df.rename(columns = {'COUNTYNAME': 'county_name', 'COUNTYFP': 'county_id', 'STATEFP': 'state_id'}, inplace = True)\n",
    "\n",
    "combined_df.to_csv(\"data/locations.csv\")\n",
    "\n",
    "\n",
    "# Add separate location data into dict for syncing\n",
    "\n",
    "location_dict = {\n",
    "    'CA': ca_codes,\n",
    "    'TX': tx_codes,\n",
    "    'GA': ga_codes,\n",
    "    'FL': fl_codes,\n",
    "    'AZ': az_codes\n",
    "}\n",
    "\n",
    "# Load population data from census website\n",
    "\n",
    "\"\"\"state_df = pd.read_csv(\"/your path.csv\")\"\"\"\n",
    "\n",
    "\n",
    "# Add states to dictionary to make transforming easier\n",
    "\n",
    "state_population_dict = {}\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "for state in states:\n",
    "    state_population_dict[state] = pd.read_csv(f\"data/{state} City population estimates.csv\")\n",
    "\n",
    "\n",
    "# Sync location and population data to standardize state, county and place codes\n",
    "\n",
    "%run Scripts/sync.py\n",
    "\n",
    "sync(states, state_population_dict, location_dict)\n",
    "\n",
    "\n",
    "# Combine state_population_dict for loading\n",
    "\n",
    "population_df = pd.concat(state_population_dict.values(), ignore_index = True)\n",
    "\n",
    "\n",
    "# Replace any A values in official Census feature\n",
    "\n",
    "population_df['CENSUS2010POP'] = population_df['CENSUS2010POP'].apply(lambda x: 0 if x == 'A' else x)\n",
    "\n",
    "\n",
    "# Connect and load location and population data\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "\n",
    "# Location loading script\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = combined_df.columns\n",
    "\n",
    "data_to_insert = list(combined_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "insert_clause = \"\"\"\n",
    "INSERT IGNORE INTO locations (state_name, state_id, county_id, county_name, place_id, place_name)\n",
    "Values (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(insert_clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "\n",
    "\n",
    "# Population loading script\n",
    "\n",
    "data_to_insert = list(population_df[['STATE', 'COUNTYFP', 'PLACEFP', 'CENSUS2010POP', 'POPESTIMATE2011', 'POPESTIMATE2012', \n",
    "                                     'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015','POPESTIMATE2016', 'POPESTIMATE2017', \n",
    "                                     'POPESTIMATE2018','POPESTIMATE2019']].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO census (state_id, county_id, place_id, true_pop_2010, pop_estimate_2011, pop_estimate_2012, pop_estimate_2013, pop_estimate_2014, \n",
    "pop_estimate_2015, pop_estimate_2016, pop_estimate_2017, pop_estimate_2018, pop_estimate_2019)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_23384\\588213273.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "%run Scripts/connect.py\n",
    "\n",
    "query =\"\"\"\n",
    "SELECT \n",
    "    l.county_name, \n",
    "    COUNT(w.fire_id) AS total_fires, \n",
    "    SUM(w.fire_size) AS total_fire_size\n",
    "FROM wildfire w\n",
    "JOIN locations l \n",
    "    ON w.state_id = l.state_id \n",
    "    AND w.county_id = l.county_id\n",
    "WHERE YEAR(w.discovery_date) = 2015\n",
    "GROUP BY l.county_name\n",
    "ORDER BY total_fires DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_23384\\438317897.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "%run Scripts/connect.py\n",
    "\n",
    "query =\"\"\"\n",
    "SELECT \n",
    "    l.state_name, \n",
    "    YEAR(w.discovery_date) AS fire_year, \n",
    "    AVG(w.fire_size) AS avg_total_burned\n",
    "FROM wildfire w\n",
    "JOIN locations l \n",
    "    ON w.state_id = l.state_id \n",
    "GROUP BY l.state_name, YEAR(w.discovery_date)\n",
    "ORDER BY fire_year ASC, avg_total_burned DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
