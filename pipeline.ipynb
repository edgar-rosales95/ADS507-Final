{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector as mysql\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Database create or already exists\n",
      "Database schema created successfully\n"
     ]
    }
   ],
   "source": [
    "# %run Scripts/schema_creation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['OBJECTID', 'FIRE_NAME', 'DISCOVERY_DATE', 'NWCG_GENERAL_CAUSE', 'FIRE_SIZE', 'STATE', 'FIPS_NAME']\n",
    "\n",
    "# Download latest version\n",
    "wildfire_df = kagglehub.load_dataset(handle = \"behroozsohrabi/us-wildfire-records-6th-edition\", path = \"data.csv\", \n",
    "                                   adapter = KaggleDatasetAdapter.PANDAS, \n",
    "                                   pandas_kwargs={\"usecols\": use_cols, \"compression\": \"zip\"})\n",
    "\n",
    "\n",
    "# Filter states that we're keeping\n",
    "\n",
    "states_to_keep = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "wildfire_df.loc[~wildfire_df['STATE'].isin(states_to_keep), :] = None\n",
    "\n",
    "wildfire_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Convert DISCOVERY_DATE to datetime object to remove records prior to year 2000\n",
    "\n",
    "wildfire_df['DISCOVERY_DATE'] = pd.to_datetime(wildfire_df['DISCOVERY_DATE'], format = ('%m/%d/%Y'))\n",
    "\n",
    "wildfire_df = wildfire_df[wildfire_df['DISCOVERY_DATE'].dt.year > 2000]\n",
    "\n",
    "\n",
    "# Change to int for compatiabilty with Primary Key\n",
    "\n",
    "wildfire_df['OBJECTID'] = wildfire_df['OBJECTID'].astype(int)\n",
    "\n",
    "\n",
    "# Rename some columns for clarity\n",
    "\n",
    "wildfire_df = wildfire_df.rename(columns = {\"NWCG_GENERAL_CAUSE\": \"SPECIFIC_CAUSE\", \"FIPS_NAME\": \"COUNTY\"})\n",
    "\n",
    "\n",
    "# Call script to map state FIPS codes to new columns\n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "wildfire_df['state_id'] = wildfire_df['STATE'].map(state_fips_mapping)\n",
    "\n",
    "\n",
    "# Realign columns and load location data to prep for county_id mapping\n",
    "\n",
    "wildfire_df.rename(columns = {'COUNTY': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "\n",
    "# Map county codes to wildfire data\n",
    "\n",
    "wildfire_df = match_county_id(wildfire_df, locations_df)\n",
    "\n",
    "\n",
    "# Use missing values scripts to fill anything missing\n",
    "\n",
    "%run Scripts/missing_values.py\n",
    "\n",
    "missing_dict = {\n",
    "    \"FL\": 86,\n",
    "    \"GA\": 29\n",
    "}\n",
    "\n",
    "wildfire_df = fill_missing_values(wildfire_df, missing_dict, \"STATE\", \"county_id\")\n",
    "\n",
    "\n",
    "# Connect to database\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = wildfire_df.columns\n",
    "\n",
    "data_to_insert = list(wildfire_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO wildfire (fire_id, fire_name, discovery_date, cause, fire_size, state_name, county_name, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 10000\n",
    "start_index = 0\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "        cursor.executemany(clause, batch)\n",
    "        conn.commit()\n",
    "        print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994800/994800 [01:01<00:00, 16259.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully for batch 1\n",
      "Data loaded successfully for batch 2\n",
      "Data loaded successfully for batch 3\n",
      "Data loaded successfully for batch 4\n",
      "Data loaded successfully for batch 5\n",
      "Data loaded successfully for batch 6\n",
      "Data loaded successfully for batch 7\n",
      "Data loaded successfully for batch 8\n",
      "Data loaded successfully for batch 9\n",
      "Data loaded successfully for batch 10\n"
     ]
    }
   ],
   "source": [
    "# Download Zillow data\n",
    "\n",
    "\"\"\"df = pd.read_csv(\"/your path.csv\")\"\"\"\n",
    "\n",
    "\n",
    "# To parquet if needed for on-site storage or to load to github\n",
    "\n",
    "\"\"\"df.to_parquet(\"/your path.parquet)\"\"\"\n",
    "\n",
    "\n",
    "# Load and remove columns that aren't used\n",
    "\n",
    "bottom_tier_housing = pd.read_parquet('data/bottom_housing.parquet')\n",
    "top_tier_housing = pd.read_parquet('data/top_housing.parquet')\n",
    "\n",
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "\n",
    "bottom_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "top_tier_housing.drop(columns = remove_cols, inplace = True)\n",
    "\n",
    "\n",
    "# Filter out states not using and remove na values\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "bottom_tier_housing.loc[~bottom_tier_housing['State'].isin(states), :] = None\n",
    "top_tier_housing.loc[~top_tier_housing['State'].isin(states), :] = None\n",
    "\n",
    "bottom_tier_housing.dropna(inplace = True)\n",
    "top_tier_housing.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Combine the data frames and pivot long the date features and price values and change date column to date dtype\n",
    "\n",
    "housing_df = pd.concat([bottom_tier_housing, top_tier_housing], axis = 0, ignore_index = True)\n",
    "\n",
    "housing_df = housing_df.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "housing_df['Date'] = pd.to_datetime(housing_df['Date']).dt.date\n",
    "\n",
    "\n",
    "# Map state and county id codes \n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "housing_df['state_id'] = housing_df['State'].map(state_fips_mapping)\n",
    "\n",
    "housing_df.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "housing_df = match_county_id(housing_df, locations_df)\n",
    "\n",
    "\n",
    "# Connect and load data\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = housing_df.columns\n",
    "\n",
    "data_to_insert = list(housing_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO housing (region_name, state_name, county_name, assessment_date, price, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "start_index = 0\n",
    "\n",
    "try:\n",
    "    for i in range(start_index, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == '2013 (HY000): Lost connection to MySQL server during query':\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)  # Retry query\n",
    "                conn.commit()\n",
    "\n",
    "except Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in rental data\n",
    "\n",
    "rent_index = pd.read_csv(\"data/Observed Rent Index by City.csv\")\n",
    "\n",
    "\n",
    "# Remove unused columns and filter out states\n",
    "\n",
    "remove_cols = ['RegionID', 'RegionType', 'StateName', 'Metro', 'SizeRank']\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "rent_index.drop(columns = remove_cols, inplace = True)\n",
    "rent_index.loc[~rent_index['State'].isin(states), :] = None\n",
    "\n",
    "rent_index.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Pivot long date features and price values and change date column to date dtype\n",
    "\n",
    "rent_index = rent_index.melt(id_vars = ['RegionName', 'State', 'CountyName'], var_name = 'Date', value_name = 'Price')\n",
    "rent_index['Date'] = pd.to_datetime(rent_index['Date']).dt.date\n",
    "\n",
    "\n",
    "# Map state and county id codes \n",
    "\n",
    "%run Scripts/mapping.py\n",
    "\n",
    "rent_index['state_id'] = rent_index['State'].map(state_fips_mapping)\n",
    "\n",
    "rent_index.rename(columns = {'CountyName': 'county_name'}, inplace = True)\n",
    "\n",
    "locations_df = pd.read_csv(\"data/locations.csv\")\n",
    "\n",
    "rent_index = match_county_id(rent_index, locations_df)\n",
    "\n",
    "\n",
    "# Connect and load rental data into database\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = rent_index.columns\n",
    "\n",
    "data_to_insert = list(rent_index[columns].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO rentals (region_name, state_name, county_name, assessment_date, price, state_id, county_id)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 15000\n",
    "\n",
    "try:\n",
    "    for i in range(0, len(data_to_insert), batch_size):\n",
    "        batch = data_to_insert[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            cursor.executemany(clause, batch)\n",
    "            conn.commit()\n",
    "            print(f\"Data loaded successfully for batch {i // batch_size + 1}\")\n",
    "        except Error as e:\n",
    "            if e == \"2013 (HY000): Lost connection to MySQL server during query\":\n",
    "                print(\"Lost connection, attempting to reconnect...\")\n",
    "                reconnect()\n",
    "                cursor.executemany(clause, batch)\n",
    "                conn.commmit()\n",
    "except Error as err:\n",
    "    print(err)\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location and Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n",
      "Data loaded successfully\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load location data from census website\n",
    "\n",
    "ca_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st06_ca_place_by_county2020.txt', delimiter = '|')\n",
    "tx_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st48_tx_place_by_county2020.txt', delimiter = '|')\n",
    "ga_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st13_ga_place_by_county2020.txt', delimiter = '|')\n",
    "fl_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st12_fl_place_by_county2020.txt', delimiter = '|')\n",
    "az_codes = pd.read_csv('https://www2.census.gov/geo/docs/reference/codes2020/place_by_cou/st04_az_place_by_county2020.txt', delimiter = '|')\n",
    "\n",
    "\n",
    "# Combine data\n",
    "\n",
    "combined_df = pd.concat([ca_codes, tx_codes, ga_codes, fl_codes, az_codes], axis = 0, ignore_index=True)\n",
    "\n",
    "\n",
    "#Drop unused columns\n",
    "\n",
    "columns_to_drop = ['PLACENS', 'TYPE', 'CLASSFP', 'FUNCSTAT']\n",
    "\n",
    "combined_df.drop(columns = columns_to_drop, inplace = True)\n",
    "\n",
    "\n",
    "# Rename columns for mapping, and load updated data to folder for reference for other tables\n",
    "\n",
    "combined_df.rename(columns = {'COUNTYNAME': 'county_name', 'COUNTYFP': 'county_id', 'STATEFP': 'state_id'}, inplace = True)\n",
    "\n",
    "combined_df.to_csv(\"data/locations.csv\")\n",
    "\n",
    "\n",
    "# Add separate location data into dict for syncing\n",
    "\n",
    "location_dict = {\n",
    "    'CA': ca_codes,\n",
    "    'TX': tx_codes,\n",
    "    'GA': ga_codes,\n",
    "    'FL': fl_codes,\n",
    "    'AZ': az_codes\n",
    "}\n",
    "\n",
    "# Load population data from census website\n",
    "\n",
    "\"\"\"state_df = pd.read_csv(\"/your path.csv\")\"\"\"\n",
    "\n",
    "\n",
    "# Add states to dictionary to make transforming easier\n",
    "\n",
    "state_population_dict = {}\n",
    "\n",
    "states = ['CA', 'TX', 'GA', 'FL', 'AZ']\n",
    "\n",
    "for state in states:\n",
    "    state_population_dict[state] = pd.read_csv(f\"data/{state} City population estimates.csv\")\n",
    "\n",
    "\n",
    "# Sync location and population data to standardize state, county and place codes\n",
    "\n",
    "%run Scripts/sync.py\n",
    "\n",
    "sync(states, state_population_dict, location_dict)\n",
    "\n",
    "\n",
    "# Combine state_population_dict for loading\n",
    "\n",
    "population_df = pd.concat(state_population_dict.values(), ignore_index = True)\n",
    "\n",
    "\n",
    "# Replace any A values in official Census feature\n",
    "\n",
    "population_df['CENSUS2010POP'] = population_df['CENSUS2010POP'].apply(lambda x: 0 if x == 'A' else x)\n",
    "\n",
    "\n",
    "# Connect and load location and population data\n",
    "\n",
    "%run Scripts/connect.py\n",
    "\n",
    "\n",
    "# Location loading script\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "columns = combined_df.columns\n",
    "\n",
    "data_to_insert = list(combined_df[columns].itertuples(index = False, name = None))\n",
    "\n",
    "insert_clause = \"\"\"\n",
    "INSERT IGNORE INTO locations (state_name, state_id, county_id, county_name, place_id, place_name)\n",
    "Values (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(insert_clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "\n",
    "\n",
    "# Population loading script\n",
    "\n",
    "data_to_insert = list(population_df[['STATE', 'COUNTYFP', 'PLACEFP', 'CENSUS2010POP', 'POPESTIMATE2011', 'POPESTIMATE2012', \n",
    "                                     'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015','POPESTIMATE2016', 'POPESTIMATE2017', \n",
    "                                     'POPESTIMATE2018','POPESTIMATE2019']].itertuples(index = False, name = None))\n",
    "\n",
    "clause = \"\"\"\n",
    "INSERT IGNORE INTO census (state_id, county_id, place_id, true_pop_2010, pop_estimate_2011, pop_estimate_2012, pop_estimate_2013, pop_estimate_2014, \n",
    "pop_estimate_2015, pop_estimate_2016, pop_estimate_2017, pop_estimate_2018, pop_estimate_2019)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    cursor.executemany(clause, data_to_insert)\n",
    "    conn.commit()\n",
    "    print('Data loaded successfully')\n",
    "except Error as err:\n",
    "    print(f\"Data unable to be loaded: {err}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_23384\\588213273.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "%run Scripts/connect.py\n",
    "\n",
    "query =\"\"\"\n",
    "SELECT \n",
    "    l.county_name, \n",
    "    COUNT(w.fire_id) AS total_fires, \n",
    "    SUM(w.fire_size) AS total_fire_size\n",
    "FROM wildfire w\n",
    "JOIN locations l \n",
    "    ON w.state_id = l.state_id \n",
    "    AND w.county_id = l.county_id\n",
    "WHERE YEAR(w.discovery_date) = 2015\n",
    "GROUP BY l.county_name\n",
    "ORDER BY total_fires DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully connected to MySQL database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taylor\\AppData\\Local\\Temp\\ipykernel_23384\\438317897.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "%run Scripts/connect.py\n",
    "\n",
    "query =\"\"\"\n",
    "SELECT \n",
    "    l.state_name, \n",
    "    YEAR(w.discovery_date) AS fire_year, \n",
    "    AVG(w.fire_size) AS avg_total_burned\n",
    "FROM wildfire w\n",
    "JOIN locations l \n",
    "    ON w.state_id = l.state_id \n",
    "GROUP BY l.state_name, YEAR(w.discovery_date)\n",
    "ORDER BY fire_year ASC, avg_total_burned DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
